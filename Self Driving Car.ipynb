{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/self-driving-car.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <h1> Self Driving Car </h1> </center>\n",
    "\n",
    "<br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information & Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "XHFnthirwlfn"
   },
   "source": [
    "Credits: https://github.com/SullyChen/Autopilot-TensorFlow\n",
    "Research paper: End to End Learning for Self-Driving Cars by Nvidia. [https://arxiv.org/pdf/1604.07316.pdf]\n",
    "\n",
    "NVidia dataset: 72 hrs of video => 72*60*60*30 = 7,776,000 images\n",
    "Nvidia blog: https://devblogs.nvidia.com/deep-learning-self-driving-cars/\n",
    "\n",
    "\n",
    "Our Dataset: https://github.com/SullyChen/Autopilot-TensorFlow [https://drive.google.com/file/d/0B-KJCaaF7elleG1RbzVPZWV4Tlk/view]\n",
    "Size: 25 minutes = 25*60*30 = 45,000 images ~ 2.3 GB\n",
    "\n",
    "\n",
    "If you want to try on a slightly large dataset: 70 minutes of data ~ 223GB\n",
    "Refer: https://medium.com/udacity/open-sourcing-223gb-of-mountain-view-driving-data-f6b5593fbfa5\n",
    "Format: Image, latitude, longitude, gear, brake, throttle, steering angles and speed\n",
    "\n",
    "\n",
    "\n",
    "Additional Installations:\n",
    "pip3 install h5py\n",
    "\n",
    "\n",
    "AWS: https://aws.amazon.com/blogs/machine-learning/get-started-with-deep-learning-using-the-aws-deep-learning-ami/\n",
    "\n",
    "Youtube:https://www.youtube.com/watch?v=qhUvQiKec2U\n",
    "Further reading and extensions: https://medium.com/udacity/teaching-a-machine-to-steer-a-car-d73217f2492c\n",
    "More data: https://medium.com/udacity/open-sourcing-223gb-of-mountain-view-driving-data-f6b5593fbfa5\n",
    "\n",
    "<br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.misc\n",
    "from scipy import pi\n",
    "from subprocess import call\n",
    "from datetime import datetime\n",
    "from itertools import islice\n",
    "import matplotlib.pyplot as plt \n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets perform some EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data = []\n",
    "angle_data = []\n",
    "\n",
    "# Get number of images\n",
    "num_images = 0\n",
    "\n",
    "# Number of images for training\n",
    "num_train_images = 0\n",
    "\n",
    "# Number of images for testing\n",
    "num_test_images = 0\n",
    "\n",
    "def load_dataset():\n",
    "    # Read data.txt\n",
    "    with open(\"data/data.txt\") as fp:\n",
    "        for line in fp:\n",
    "            image_data.append(\"data/\" + line.split()[0])\n",
    "\n",
    "            # the paper by Nvidia uses the inverse of the turning radius,\n",
    "            # but steering wheel angle is proportional to the inverse of turning radius\n",
    "            # so the steering wheel angle in radians is used as the output       \n",
    "            angle_data.append(float(line.split()[1]) * scipy.pi / 180)\n",
    "\n",
    "def split_dataset(train_split,test_split):\n",
    "    images_to_train = image_data[:int(len(image_data) * train_split)]    \n",
    "    angles_to_train = angle_data[:int(len(image_data) * train_split)]\n",
    "\n",
    "    images_to_test = image_data[-int(len(image_data) * test_split):]\n",
    "    angles_to_test = angle_data[-int(len(image_data) * test_split):]\n",
    "    \n",
    "    return images_to_train,angles_to_train,images_to_test,angles_to_test   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images:  45406\n",
      "Total number of images for training:  36324\n",
      "Total number of images for testing:  9081\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "load_dataset()\n",
    "\n",
    "# Split dataset\n",
    "images_to_train,angles_to_train,images_to_test,angles_to_test = split_dataset(0.8,0.2)\n",
    "\n",
    "num_images = len(image_data)\n",
    "print(\"Total number of images: \",num_images)\n",
    "\n",
    "num_train_images = len(images_to_train)\n",
    "print(\"Total number of images for training: \",num_train_images)\n",
    "\n",
    "num_test_images = len(images_to_test)\n",
    "print(\"Total number of images for testing: \",num_test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PDF of train and test angle values. \n",
    "plt.figure(figsize=(12,8))\n",
    "plt.hist(angles_to_train, bins=50, density=1, color='green', histtype ='step',label=\"Train Angles\")\n",
    "plt.hist(angles_to_test, bins=50, density=1, color='red', histtype ='step',label=\"Test Angles\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "\n",
    "By observing above histogram plot, we can apply the simple Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Base line Model: y_test_pred = mean(y_train_i) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_MSE(MEAN):0.191142\n"
     ]
    }
   ],
   "source": [
    "#Model 1: Base line Model: y_test_pred = mean(y_train_i) \n",
    "train_mean_angle = np.mean(angles_to_train)\n",
    "\n",
    "print('Test_MSE(MEAN):%f' % np.mean(np.square(angles_to_test - train_mean_angle)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create End to End Model\n",
    "Credits End to End Learning for Self-Driving Cars by Nvidia. [https://arxiv.org/pdf/1604.07316.pdf]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1 - Configuration Used:\n",
    "Train/Test Split: 80:20 <br/>\n",
    "Dropout : 0.80 <br/>\n",
    "AdamOptimezer Value: 1e-4 <br/>\n",
    "Activation Function: atan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W, stride):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, stride, stride, 1], padding='VALID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_image = tf.placeholder(tf.float32, shape=[None, 66, 200, 3],name=\"true_image\")\n",
    "true_angle = tf.placeholder(tf.float32, shape=[None, 1],name=\"true_angle\")\n",
    "\n",
    "x_image = true_image\n",
    "\n",
    "#first convolutional layer\n",
    "W_conv1 = weight_variable([5, 5, 3, 24])\n",
    "b_conv1 = bias_variable([24])\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1, 2) + b_conv1)\n",
    "\n",
    "#second convolutional layer\n",
    "W_conv2 = weight_variable([5, 5, 24, 36])\n",
    "b_conv2 = bias_variable([36])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_conv1, W_conv2, 2) + b_conv2)\n",
    "\n",
    "#third convolutional layer\n",
    "W_conv3 = weight_variable([5, 5, 36, 48])\n",
    "b_conv3 = bias_variable([48])\n",
    "\n",
    "h_conv3 = tf.nn.relu(conv2d(h_conv2, W_conv3, 2) + b_conv3)\n",
    "\n",
    "#fourth convolutional layer\n",
    "W_conv4 = weight_variable([3, 3, 48, 64])\n",
    "b_conv4 = bias_variable([64])\n",
    "\n",
    "h_conv4 = tf.nn.relu(conv2d(h_conv3, W_conv4, 1) + b_conv4)\n",
    "\n",
    "#fifth convolutional layer\n",
    "W_conv5 = weight_variable([3, 3, 64, 64])\n",
    "b_conv5 = bias_variable([64])\n",
    "\n",
    "h_conv5 = tf.nn.relu(conv2d(h_conv4, W_conv5, 1) + b_conv5)\n",
    "\n",
    "#FCL 1\n",
    "W_fc1 = weight_variable([1152, 1164])\n",
    "b_fc1 = bias_variable([1164])\n",
    "\n",
    "h_conv5_flat = tf.reshape(h_conv5, [-1, 1152])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_conv5_flat, W_fc1) + b_fc1)\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32,name=\"keep_prob\")\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "#FCL 2\n",
    "W_fc2 = weight_variable([1164, 100])\n",
    "b_fc2 = bias_variable([100])\n",
    "\n",
    "h_fc2 = tf.nn.relu(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "\n",
    "h_fc2_drop = tf.nn.dropout(h_fc2, keep_prob)\n",
    "\n",
    "#FCL 3\n",
    "W_fc3 = weight_variable([100, 50])\n",
    "b_fc3 = bias_variable([50])\n",
    "\n",
    "h_fc3 = tf.nn.relu(tf.matmul(h_fc2_drop, W_fc3) + b_fc3)\n",
    "\n",
    "h_fc3_drop = tf.nn.dropout(h_fc3, keep_prob)\n",
    "\n",
    "#FCL 3\n",
    "W_fc4 = weight_variable([50, 10])\n",
    "b_fc4 = bias_variable([10])\n",
    "\n",
    "h_fc4 = tf.nn.relu(tf.matmul(h_fc3_drop, W_fc4) + b_fc4)\n",
    "\n",
    "h_fc4_drop = tf.nn.dropout(h_fc4, keep_prob)\n",
    "\n",
    "#Output\n",
    "W_fc5 = weight_variable([10, 1])\n",
    "b_fc5 = bias_variable([1])\n",
    "\n",
    "# atan activation function with scaling\n",
    "predicted_angle = tf.multiply(tf.atan(tf.matmul(h_fc4_drop, W_fc5) + b_fc5), 2)\n",
    "predicted_angle = tf.identity(predicted_angle,name=\"predicted_angle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2 - Configuration Used:\n",
    "Train/Test Split: 70:30 <br/>\n",
    "Dropout : 0.50 <br/>\n",
    "AdamOptimezer Value: 1e-3 <br/>\n",
    "Activation Function: linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_image_ln = tf.placeholder(tf.float32, shape=[None, 66, 200, 3],name=\"true_image_ln\")\n",
    "true_angle_ln = tf.placeholder(tf.float32, shape=[None, 1],name=\"true_angle_ln\")\n",
    "\n",
    "x_image_ln = true_image_ln\n",
    "\n",
    "#first convolutional layer\n",
    "W_conv1_ln = weight_variable([5, 5, 3, 24])\n",
    "b_conv1_ln = bias_variable([24])\n",
    "\n",
    "h_conv1_ln = tf.nn.relu(conv2d(x_image_ln, W_conv1_ln, 2) + b_conv1_ln)\n",
    "\n",
    "#second convolutional layer\n",
    "W_conv2_ln = weight_variable([5, 5, 24, 36])\n",
    "b_conv2_ln = bias_variable([36])\n",
    "\n",
    "h_conv2_ln = tf.nn.relu(conv2d(h_conv1_ln, W_conv2_ln, 2) + b_conv2_ln)\n",
    "\n",
    "#third convolutional layer\n",
    "W_conv3_ln = weight_variable([5, 5, 36, 48])\n",
    "b_conv3_ln = bias_variable([48])\n",
    "\n",
    "h_conv3_ln = tf.nn.relu(conv2d(h_conv2_ln, W_conv3_ln, 2) + b_conv3_ln)\n",
    "\n",
    "#fourth convolutional layer\n",
    "W_conv4_ln = weight_variable([3, 3, 48, 64])\n",
    "b_conv4_ln = bias_variable([64])\n",
    "\n",
    "h_conv4_ln = tf.nn.relu(conv2d(h_conv3_ln, W_conv4_ln, 1) + b_conv4_ln)\n",
    "\n",
    "#fifth convolutional layer\n",
    "W_conv5_ln = weight_variable([3, 3, 64, 64])\n",
    "b_conv5_ln = bias_variable([64])\n",
    "\n",
    "h_conv5_ln = tf.nn.relu(conv2d(h_conv4_ln, W_conv5_ln, 1) + b_conv5_ln)\n",
    "\n",
    "#FCL 1\n",
    "W_fc1_ln = weight_variable([1152, 1164])\n",
    "b_fc1_ln = bias_variable([1164])\n",
    "\n",
    "h_conv5_flat_ln = tf.reshape(h_conv5_ln, [-1, 1152])\n",
    "h_fc1_ln = tf.nn.relu(tf.matmul(h_conv5_flat_ln, W_fc1_ln) + b_fc1_ln)\n",
    "\n",
    "keep_prob_ln = tf.placeholder(tf.float32,name=\"keep_prob_ln\")\n",
    "h_fc1_drop_ln = tf.nn.dropout(h_fc1_ln, keep_prob_ln)\n",
    "\n",
    "#FCL 2\n",
    "W_fc2_ln = weight_variable([1164, 100])\n",
    "b_fc2_ln = bias_variable([100])\n",
    "\n",
    "h_fc2_ln = tf.nn.relu(tf.matmul(h_fc1_drop_ln, W_fc2_ln) + b_fc2_ln)\n",
    "\n",
    "h_fc2_drop_ln = tf.nn.dropout(h_fc2_ln, keep_prob_ln)\n",
    "\n",
    "#FCL 3\n",
    "W_fc3_ln = weight_variable([100, 50])\n",
    "b_fc3_ln = bias_variable([50])\n",
    "\n",
    "h_fc3_ln = tf.nn.relu(tf.matmul(h_fc2_drop_ln, W_fc3_ln) + b_fc3_ln)\n",
    "\n",
    "h_fc3_drop_ln = tf.nn.dropout(h_fc3_ln, keep_prob_ln)\n",
    "\n",
    "#FCL 3\n",
    "W_fc4_ln = weight_variable([50, 10])\n",
    "b_fc4_ln = bias_variable([10])\n",
    "\n",
    "h_fc4_ln = tf.nn.relu(tf.matmul(h_fc3_drop_ln, W_fc4_ln) + b_fc4_ln)\n",
    "\n",
    "h_fc4_drop_ln = tf.nn.dropout(h_fc4_ln, keep_prob_ln)\n",
    "\n",
    "#Output\n",
    "W_fc5_ln = weight_variable([10, 1])\n",
    "b_fc5_ln = bias_variable([1])\n",
    "\n",
    "# linear activation function\n",
    "predicted_angle_ln = tf.matmul(h_fc4_drop_ln, W_fc5_ln) + b_fc5_ln\n",
    "predicted_angle_ln = tf.identity(predicted_angle_ln,name=\"predicted_angle_ln\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "\n",
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 with 'atan' activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#points to the end of the last batch\n",
    "train_batch_pointer = 0\n",
    "test_batch_pointer = 0\n",
    "\n",
    "# Utility Functions\n",
    "def LoadTrainBatch(batch_size):\n",
    "    global train_batch_pointer\n",
    "    x_out = []\n",
    "    y_out = []\n",
    "    for i in range(0, batch_size):\n",
    "        x_out.append(scipy.misc.imresize(scipy.misc.imread(images_to_train[(train_batch_pointer + i) % num_train_images])[-150:], \n",
    "                                         [66, 200]) / 255.0)\n",
    "        y_out.append([angles_to_train[(train_batch_pointer + i) % num_train_images]])\n",
    "    train_batch_pointer += batch_size\n",
    "    return x_out, y_out\n",
    "\n",
    "def LoadTestBatch(batch_size): \n",
    "    global test_batch_pointer\n",
    "    x_out = []\n",
    "    y_out = []\n",
    "    for i in range(0, batch_size):\n",
    "        x_out.append(scipy.misc.imresize(scipy.misc.imread(images_to_test[(test_batch_pointer + i) % num_test_images])[-150:], \n",
    "                                         [66, 200]) / 255.0)\n",
    "        y_out.append([angles_to_test[(test_batch_pointer + i) % num_test_images]])\n",
    "    test_batch_pointer += batch_size\n",
    "    return x_out, y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGDIR = './models/atan/'\n",
    "\n",
    "# Lets start the tensorflow session\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let the model learn itself...\n",
      "\n",
      "Epoch: 0, Step: 450, Loss: 4.36189\n",
      "Model saved in file: ./models/atan/model_atan.ckpt\n",
      "\n",
      "Epoch: 1, Step: 550, Loss: 3.74218\n",
      "Model saved in file: ./models/atan/model_atan.ckpt\n",
      "\n",
      "Epoch: 2, Step: 650, Loss: 2.72663\n",
      "Model saved in file: ./models/atan/model_atan.ckpt\n",
      "\n",
      "Epoch: 3, Step: 750, Loss: 2.4868\n",
      "Model saved in file: ./models/atan/model_atan.ckpt\n",
      "\n",
      "Epoch: 4, Step: 850, Loss: 2.03206\n",
      "Model saved in file: ./models/atan/model_atan.ckpt\n",
      "\n",
      "Epoch: 5, Step: 950, Loss: 1.91966\n",
      "Model saved in file: ./models/atan/model_atan.ckpt\n",
      "\n",
      "Epoch: 6, Step: 1050, Loss: 1.54203\n",
      "Model saved in file: ./models/atan/model_atan.ckpt\n",
      "\n",
      "Epoch: 7, Step: 1150, Loss: 1.42276\n",
      "Model saved in file: ./models/atan/model_atan.ckpt\n",
      "\n",
      "Epoch: 8, Step: 1250, Loss: 1.20319\n",
      "Model saved in file: ./models/atan/model_atan.ckpt\n",
      "\n",
      "Epoch: 9, Step: 1350, Loss: 1.03368\n",
      "Model saved in file: ./models/atan/model_atan.ckpt\n",
      "\n",
      "Epoch: 10, Step: 1450, Loss: 0.895972\n",
      "Model saved in file: ./models/atan/model_atan.ckpt\n",
      "\n",
      "Epoch: 11, Step: 1550, Loss: 0.870298\n",
      "Model saved in file: ./models/atan/model_atan.ckpt\n",
      "\n",
      "Epoch: 12, Step: 1650, Loss: 0.721526\n",
      "Model saved in file: ./models/atan/model_atan.ckpt\n",
      "\n",
      "Epoch: 13, Step: 1750, Loss: 1.05005\n",
      "Model saved in file: ./models/atan/model_atan.ckpt\n",
      "\n",
      "Epoch: 14, Step: 1850, Loss: 0.566407\n",
      "Model saved in file: ./models/atan/model_atan.ckpt\n",
      "\n",
      "Epoch: 15, Step: 1950, Loss: 0.581275\n",
      "Model saved in file: ./models/atan/model_atan.ckpt\n",
      "\n",
      "Epoch: 16, Step: 2050, Loss: 0.484599\n",
      "Model saved in file: ./models/atan/model_atan.ckpt\n",
      "\n",
      "Epoch: 17, Step: 2150, Loss: 0.513947\n",
      "Model saved in file: ./models/atan/model_atan.ckpt\n",
      "\n",
      "Epoch: 18, Step: 2250, Loss: 0.403349\n",
      "Model saved in file: ./models/atan/model_atan.ckpt\n",
      "\n",
      "Epoch: 19, Step: 2350, Loss: 0.375758\n",
      "Model saved in file: ./models/atan/model_atan.ckpt\n",
      "\n",
      "Epoch: 20, Step: 2450, Loss: 0.854309\n",
      "Model saved in file: ./models/atan/model_atan.ckpt\n",
      "\n",
      "Epoch: 21, Step: 2550, Loss: 0.337236\n",
      "Model saved in file: ./models/atan/model_atan.ckpt\n",
      "\n",
      "Epoch: 22, Step: 2650, Loss: 1.22638\n",
      "Model saved in file: ./models/atan/model_atan.ckpt\n",
      "\n",
      "Epoch: 23, Step: 2750, Loss: 0.322755\n",
      "Model saved in file: ./models/atan/model_atan.ckpt\n",
      "\n",
      "Epoch: 24, Step: 2850, Loss: 5.02715\n",
      "Model saved in file: ./models/atan/model_atan.ckpt\n",
      "\n",
      "Epoch: 25, Step: 2950, Loss: 0.296385\n",
      "Model saved in file: ./models/atan/model_atan.ckpt\n",
      "\n",
      "Epoch: 26, Step: 3050, Loss: 0.351221\n",
      "Model saved in file: ./models/atan/model_atan.ckpt\n",
      "\n",
      "Epoch: 27, Step: 3150, Loss: 0.335083\n",
      "Model saved in file: ./models/atan/model_atan.ckpt\n",
      "\n",
      "Epoch: 28, Step: 3250, Loss: 3.30626\n",
      "Model saved in file: ./models/atan/model_atan.ckpt\n",
      "\n",
      "Epoch: 29, Step: 3350, Loss: 0.26926\n",
      "Model saved in file: ./models/atan/model_atan.ckpt\n",
      "\n",
      "Run the command line:\n",
      "--> tensorboard --logdir=./logs \n",
      "Then open http://0.0.0.0:6006/ into your web browser\n",
      "\n",
      "Time taken to train the model:  4:16:20.736450\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "print(\"Let the model learn itself...\")\n",
    "print()\n",
    "\n",
    "L2NormConst = 0.001\n",
    "\n",
    "train_vars = tf.trainable_variables()\n",
    "\n",
    "loss = tf.reduce_mean(tf.square(tf.subtract(true_angle, predicted_angle))) + tf.add_n([tf.nn.l2_loss(v) for v in train_vars]) * L2NormConst\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# create a summary to monitor cost tensor\n",
    "tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "# merge all summaries into a single op\n",
    "merged_summary_op =  tf.summary.merge_all()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# op to write logs to Tensorboard\n",
    "logs_path = './logs'\n",
    "summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "\n",
    "epochs = 30\n",
    "batch_size = 100\n",
    "\n",
    "# train over the dataset about 30 times\n",
    "previous_i = 0\n",
    "previous_loss = 0\n",
    "for epoch in range(epochs):\n",
    "    for i in range(int(num_images/batch_size)):        \n",
    "        xs, ys = LoadTrainBatch(batch_size)\n",
    "        train_step.run(feed_dict={true_image: xs, true_angle: ys, keep_prob: 0.80})\n",
    "        if i % 10 == 0:            \n",
    "            xs, ys = LoadTestBatch(batch_size)\n",
    "            loss_value = loss.eval(feed_dict={true_image:xs, true_angle: ys, keep_prob: 1.0})\n",
    "            previous_loss = loss_value\n",
    "            previous_i = i\n",
    "            # print(\"Epoch: %d, Step: %d, Loss: %g\" % (epoch, epoch * batch_size + i, loss_value))\n",
    "\n",
    "        # write logs at every iteration\n",
    "        summary = merged_summary_op.eval(feed_dict={true_image:xs, true_angle: ys, keep_prob: 1.0})\n",
    "        summary_writer.add_summary(summary, epoch * num_images/batch_size + i)\n",
    "\n",
    "        if i % batch_size == 0:\n",
    "            if not os.path.exists(LOGDIR):\n",
    "                os.makedirs(LOGDIR)            \n",
    "            checkpoint_path = os.path.join(LOGDIR, \"model_atan.ckpt\")\n",
    "            filename = saver.save(sess, checkpoint_path)    \n",
    "    print(\"Epoch: %d, Step: %d, Loss: %g\" % (epoch, epoch * batch_size + previous_i, previous_loss)) \n",
    "    print(\"Model saved in file: %s\" % filename)\n",
    "    print()\n",
    "\n",
    "print(\"Run the command line:\\n\" \\\n",
    "          \"--> tensorboard --logdir=./logs \" \\\n",
    "          \"\\nThen open http://0.0.0.0:6006/ into your web browser\")\n",
    "\n",
    "print(\"\\nTime taken to train the model: \",datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets close the tensorflow session\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2  with 'linear' activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images:  45406\n",
      "Total number of images for training:  31784\n",
      "Total number of images for testing:  13621\n"
     ]
    }
   ],
   "source": [
    "# Split dataset\n",
    "images_to_train,angles_to_train,images_to_test,angles_to_test = split_dataset(0.7,0.3)\n",
    "\n",
    "num_images = len(image_data)\n",
    "print(\"Total number of images: \",num_images)\n",
    "\n",
    "num_train_images = len(images_to_train)\n",
    "print(\"Total number of images for training: \",num_train_images)\n",
    "\n",
    "num_test_images = len(images_to_test)\n",
    "print(\"Total number of images for testing: \",num_test_images)\n",
    "\n",
    "# Reset the pointers\n",
    "train_batch_pointer = 0\n",
    "test_batch_pointer = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGDIR = './models/linear/'\n",
    "\n",
    "# Lets start the tensorflow session\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let the model learn itself...\n",
      "\n",
      "Epoch: 0, Step: 450, Loss: 1.94443\n",
      "Model saved in file: ./models/linear/model_linear.ckpt\n",
      "\n",
      "Epoch: 1, Step: 550, Loss: 0.99808\n",
      "Model saved in file: ./models/linear/model_linear.ckpt\n",
      "\n",
      "Epoch: 2, Step: 650, Loss: 0.585102\n",
      "Model saved in file: ./models/linear/model_linear.ckpt\n",
      "\n",
      "Epoch: 3, Step: 750, Loss: 0.930208\n",
      "Model saved in file: ./models/linear/model_linear.ckpt\n",
      "\n",
      "Epoch: 4, Step: 850, Loss: 0.219191\n",
      "Model saved in file: ./models/linear/model_linear.ckpt\n",
      "\n",
      "Epoch: 5, Step: 950, Loss: 0.254192\n",
      "Model saved in file: ./models/linear/model_linear.ckpt\n",
      "\n",
      "Epoch: 6, Step: 1050, Loss: 0.157647\n",
      "Model saved in file: ./models/linear/model_linear.ckpt\n",
      "\n",
      "Epoch: 7, Step: 1150, Loss: 0.0659127\n",
      "Model saved in file: ./models/linear/model_linear.ckpt\n",
      "\n",
      "Epoch: 8, Step: 1250, Loss: 0.354662\n",
      "Model saved in file: ./models/linear/model_linear.ckpt\n",
      "\n",
      "Epoch: 9, Step: 1350, Loss: 0.0425358\n",
      "Model saved in file: ./models/linear/model_linear.ckpt\n",
      "\n",
      "Epoch: 10, Step: 1450, Loss: 0.0224573\n",
      "Model saved in file: ./models/linear/model_linear.ckpt\n",
      "\n",
      "Epoch: 11, Step: 1550, Loss: 0.190558\n",
      "Model saved in file: ./models/linear/model_linear.ckpt\n",
      "\n",
      "Epoch: 12, Step: 1650, Loss: 0.57656\n",
      "Model saved in file: ./models/linear/model_linear.ckpt\n",
      "\n",
      "Epoch: 13, Step: 1750, Loss: 0.00663601\n",
      "Model saved in file: ./models/linear/model_linear.ckpt\n",
      "\n",
      "Epoch: 14, Step: 1850, Loss: 0.22067\n",
      "Model saved in file: ./models/linear/model_linear.ckpt\n",
      "\n",
      "Epoch: 15, Step: 1950, Loss: 0.749397\n",
      "Model saved in file: ./models/linear/model_linear.ckpt\n",
      "\n",
      "Epoch: 16, Step: 2050, Loss: 0.00279485\n",
      "Model saved in file: ./models/linear/model_linear.ckpt\n",
      "\n",
      "Epoch: 17, Step: 2150, Loss: 0.0465793\n",
      "Model saved in file: ./models/linear/model_linear.ckpt\n",
      "\n",
      "Epoch: 18, Step: 2250, Loss: 0.0885663\n",
      "Model saved in file: ./models/linear/model_linear.ckpt\n",
      "\n",
      "Epoch: 19, Step: 2350, Loss: 0.007188\n",
      "Model saved in file: ./models/linear/model_linear.ckpt\n",
      "\n",
      "Epoch: 20, Step: 2450, Loss: 0.0490835\n",
      "Model saved in file: ./models/linear/model_linear.ckpt\n",
      "\n",
      "Epoch: 21, Step: 2550, Loss: 0.00709422\n",
      "Model saved in file: ./models/linear/model_linear.ckpt\n",
      "\n",
      "Epoch: 22, Step: 2650, Loss: 1.06157\n",
      "Model saved in file: ./models/linear/model_linear.ckpt\n",
      "\n",
      "Epoch: 23, Step: 2750, Loss: 0.700144\n",
      "Model saved in file: ./models/linear/model_linear.ckpt\n",
      "\n",
      "Epoch: 24, Step: 2850, Loss: 0.0139507\n",
      "Model saved in file: ./models/linear/model_linear.ckpt\n",
      "\n",
      "Epoch: 25, Step: 2950, Loss: 0.0396665\n",
      "Model saved in file: ./models/linear/model_linear.ckpt\n",
      "\n",
      "Epoch: 26, Step: 3050, Loss: 0.487887\n",
      "Model saved in file: ./models/linear/model_linear.ckpt\n",
      "\n",
      "Epoch: 27, Step: 3150, Loss: 0.0514921\n",
      "Model saved in file: ./models/linear/model_linear.ckpt\n",
      "\n",
      "Epoch: 28, Step: 3250, Loss: 0.0176178\n",
      "Model saved in file: ./models/linear/model_linear.ckpt\n",
      "\n",
      "Epoch: 29, Step: 3350, Loss: 0.442589\n",
      "Model saved in file: ./models/linear/model_linear.ckpt\n",
      "\n",
      "Run the command line:\n",
      "--> tensorboard --logdir=./logs \n",
      "Then open http://0.0.0.0:6006/ into your web browser\n",
      "\n",
      "Time taken to train the model:  4:07:49.696838\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "print(\"Let the model learn itself...\")\n",
    "print()\n",
    "\n",
    "L2NormConst = 0.001\n",
    "\n",
    "train_vars = tf.trainable_variables()\n",
    "\n",
    "loss = tf.reduce_mean(tf.square(tf.subtract(true_angle_ln, predicted_angle_ln))) + tf.add_n([tf.nn.l2_loss(v) for v in train_vars]) * L2NormConst\n",
    "train_step = tf.train.AdamOptimizer(1e-3).minimize(loss)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# create a summary to monitor cost tensor\n",
    "tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "# merge all summaries into a single op\n",
    "merged_summary_op =  tf.summary.merge_all()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# op to write logs to Tensorboard\n",
    "logs_path = './logs'\n",
    "summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "\n",
    "epochs = 30\n",
    "batch_size = 100\n",
    "\n",
    "# train over the dataset about 30 times\n",
    "previous_i = 0\n",
    "previous_loss = 0\n",
    "for epoch in range(epochs):\n",
    "    for i in range(int(num_images/batch_size)):        \n",
    "        xs, ys = LoadTrainBatch(batch_size)\n",
    "        train_step.run(feed_dict={true_image_ln: xs, true_angle_ln: ys, keep_prob_ln: 0.50})\n",
    "        if i % 10 == 0:            \n",
    "            xs, ys = LoadTestBatch(batch_size)\n",
    "            loss_value = loss.eval(feed_dict={true_image_ln:xs, true_angle_ln: ys, keep_prob_ln: 1.0})\n",
    "            previous_loss = loss_value\n",
    "            previous_i = i\n",
    "            # print(\"Epoch: %d, Step: %d, Loss: %g\" % (epoch, epoch * batch_size + i, loss_value))\n",
    "\n",
    "        # write logs at every iteration\n",
    "        summary = merged_summary_op.eval(feed_dict={true_image_ln:xs, true_angle_ln: ys, keep_prob_ln: 1.0})\n",
    "        summary_writer.add_summary(summary, epoch * num_images/batch_size + i)\n",
    "\n",
    "        if i % batch_size == 0:\n",
    "            if not os.path.exists(LOGDIR):\n",
    "                os.makedirs(LOGDIR)            \n",
    "            checkpoint_path = os.path.join(LOGDIR, \"model_linear.ckpt\")\n",
    "            filename = saver.save(sess, checkpoint_path)    \n",
    "    print(\"Epoch: %d, Step: %d, Loss: %g\" % (epoch, epoch * batch_size + previous_i, previous_loss)) \n",
    "    print(\"Model saved in file: %s\" % filename)\n",
    "    print()\n",
    "\n",
    "print(\"Run the command line:\\n\" \\\n",
    "          \"--> tensorboard --logdir=./logs \" \\\n",
    "          \"\\nThen open http://0.0.0.0:6006/ into your web browser\")\n",
    "\n",
    "print(\"\\nTime taken to train the model: \",datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets close the tensorflow session\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "\n",
    "## Model Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: \n",
    "1. To run model 1, open command prompt or terminal and type 'pyhton3 run_atan.py'\n",
    "2. To run model 2, open command prompt or terminal and type 'pyhton3 run_linear.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Conclusion </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1 peforms descently with almost correct steering angles, whereas Model 2 does not perform well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating perfect self driving car model is very hard\n",
    "#### But still we have managed to develope simple/minimilistic model, which predicts the steering wheel angle."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "Self_driving_car.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
